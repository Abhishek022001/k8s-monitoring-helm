// Destination: otlpGateway (otlp)
otelcol.receiver.prometheus "otlpgateway" {
  output {
    metrics = [otelcol.processor.transform.otlpgateway.input]
  }
}
otelcol.receiver.loki "otlpgateway" {
  output {
    logs = [otelcol.processor.transform.otlpgateway.input]
  }
}
otelcol.auth.basic "otlpgateway" {
  username = nonsensitive(remote.kubernetes.secret.otlpgateway.data["username"])
  password = remote.kubernetes.secret.otlpgateway.data["password"]
}

otelcol.processor.transform "otlpgateway" {
  error_mode = "ignore"
  metric_statements {
    context = "resource"
    statements = ["set(attributes[\"k8s.cluster.name\"], \"all-features-cluster\") where attributes[\"k8s.cluster.name\"] == nil"]
  }
  log_statements {
    context = "resource"
    statements = ["set(attributes[\"k8s.cluster.name\"], \"all-features-cluster\") where attributes[\"k8s.cluster.name\"] == nil"]
  }
  trace_statements {
    context = "resource"
    statements = ["set(attributes[\"k8s.cluster.name\"], \"all-features-cluster\") where attributes[\"k8s.cluster.name\"] == nil"]
  }

  output {
    metrics = [otelcol.exporter.otlp.otlpgateway.input]
    logs = [otelcol.exporter.otlp.otlpgateway.input]
    traces = [otelcol.exporter.otlp.otlpgateway.input]
  }
}
otelcol.exporter.otlp "otlpgateway" {
  client {
    endpoint = "https://otlp.example.com:4317/v1/traces"
    headers = {
      "X-Scope-OrgID" = nonsensitive(remote.kubernetes.secret.otlpgateway.data["tenantId"]),
    }
    tls {
      insecure = false
      insecure_skip_verify = false
      ca_pem = nonsensitive(remote.kubernetes.secret.otlpgateway.data["ca"])
      cert_pem = nonsensitive(remote.kubernetes.secret.otlpgateway.data["cert"])
      key_pem = remote.kubernetes.secret.otlpgateway.data["key"]
    }
  }
}

remote.kubernetes.secret "otlpgateway" {
  name      = "otlpgateway-ko-k8s-monitoring"
  namespace = "default"
}

// Feature: Annotation Autodiscovery
declare "annotation_autodiscovery" {
  argument "metrics_destinations" {
    comment = "Must be a list of metric destinations where collected metrics should be forwarded to"
  }

  discovery.kubernetes "pods" {
    role = "pod"
  }

  discovery.relabel "annotation_autodiscovery_pods" {
    targets = discovery.kubernetes.pods.targets
    rule {
      source_labels = ["__meta_kubernetes_pod_annotation_k8s_grafana_com_scrape"]
      regex = "true"
      action = "keep"
    }
    rule {
      source_labels = ["__meta_kubernetes_pod_annotation_k8s_grafana_com_job"]
      action = "replace"
      target_label = "job"
    }
    rule {
      source_labels = ["__meta_kubernetes_pod_annotation_k8s_grafana_com_instance"]
      action = "replace"
      target_label = "instance"
    }
    rule {
      source_labels = ["__meta_kubernetes_pod_annotation_k8s_grafana_com_metrics_path"]
      action = "replace"
      target_label = "__metrics_path__"
    }

    // Choose the pod port
    // The discovery generates a target for each declared container port of the pod.
    // If the metricsPortName annotation has value, keep only the target where the port name matches the one of the annotation.
    rule {
      source_labels = ["__meta_kubernetes_pod_container_port_name"]
      target_label = "__tmp_port"
    }
    rule {
      source_labels = ["__meta_kubernetes_pod_annotation_k8s_grafana_com_metrics_portName"]
      regex = "(.+)"
      target_label = "__tmp_port"
    }
    rule {
      source_labels = ["__meta_kubernetes_pod_container_port_name"]
      action = "keepequal"
      target_label = "__tmp_port"
    }

    // If the metrics port number annotation has a value, override the target address to use it, regardless whether it is
    // one of the declared ports on that Pod.
    rule {
      source_labels = ["__meta_kubernetes_pod_annotation_k8s_grafana_com_metrics_portNumber", "__meta_kubernetes_pod_ip"]
      regex = "(\\d+);(([A-Fa-f0-9]{1,4}::?){1,7}[A-Fa-f0-9]{1,4})"
      replacement = "[$2]:$1" // IPv6
      target_label = "__address__"
    }
    rule {
      source_labels = ["__meta_kubernetes_pod_annotation_k8s_grafana_com_metrics_portNumber", "__meta_kubernetes_pod_ip"]
      regex = "(\\d+);((([0-9]+?)(\\.|$)){4})" // IPv4, takes priority over IPv6 when both exists
      replacement = "$2:$1"
      target_label = "__address__"
    }

    rule {
      source_labels = ["__meta_kubernetes_pod_annotation_k8s_grafana_com_metrics_scheme"]
      action = "replace"
      target_label = "__scheme__"
    }

    rule {
      source_labels = ["__meta_kubernetes_pod_annotation_k8s_grafana_com_metrics_scrapeInterval"]
      action = "replace"
      target_label = "__scrape_interval__"
    }
  }

  discovery.kubernetes "services" {
    role = "service"
  }

  discovery.relabel "annotation_autodiscovery_services" {
    targets = discovery.kubernetes.services.targets
    rule {
      source_labels = ["__meta_kubernetes_service_annotation_k8s_grafana_com_scrape"]
      regex = "true"
      action = "keep"
    }
    rule {
      source_labels = ["__meta_kubernetes_service_annotation_k8s_grafana_com_job"]
      action = "replace"
      target_label = "job"
    }
    rule {
      source_labels = ["__meta_kubernetes_service_annotation_k8s_grafana_com_instance"]
      action = "replace"
      target_label = "instance"
    }
    rule {
      source_labels = ["__meta_kubernetes_service_annotation_k8s_grafana_com_metrics_path"]
      action = "replace"
      target_label = "__metrics_path__"
    }

    // Choose the service port
    rule {
      source_labels = ["__meta_kubernetes_service_port_name"]
      target_label = "__tmp_port"
    }
    rule {
      source_labels = ["__meta_kubernetes_service_annotation_k8s_grafana_com_metrics_portName"]
      regex = "(.+)"
      target_label = "__tmp_port"
    }
    rule {
      source_labels = ["__meta_kubernetes_service_port_name"]
      action = "keepequal"
      target_label = "__tmp_port"
    }

    rule {
      source_labels = ["__meta_kubernetes_service_port_number"]
      target_label = "__tmp_port"
    }
    rule {
      source_labels = ["__meta_kubernetes_service_annotation_k8s_grafana_com_metrics_portNumber"]
      regex = "(.+)"
      target_label = "__tmp_port"
    }
    rule {
      source_labels = ["__meta_kubernetes_service_port_number"]
      action = "keepequal"
      target_label = "__tmp_port"
    }

    rule {
      source_labels = ["__meta_kubernetes_service_annotation_k8s_grafana_com_metrics_scheme"]
      action = "replace"
      target_label = "__scheme__"
    }

    rule {
      source_labels = ["__meta_kubernetes_service_annotation_k8s_grafana_com_metrics_scrapeInterval"]
      action = "replace"
      target_label = "__scrape_interval__"
    }
  }

  discovery.relabel "annotation_autodiscovery_http" {
    targets = concat(discovery.relabel.annotation_autodiscovery_pods.output, discovery.relabel.annotation_autodiscovery_services.output)
    rule {
      source_labels = ["__scheme__"]
      regex = "https"
      action = "drop"
    }
  }

  discovery.relabel "annotation_autodiscovery_https" {
    targets = concat(discovery.relabel.annotation_autodiscovery_pods.output, discovery.relabel.annotation_autodiscovery_services.output)
    rule {
      source_labels = ["__scheme__"]
      regex = "https"
      action = "keep"
    }
  }

  prometheus.scrape "annotation_autodiscovery_http" {
    targets = discovery.relabel.annotation_autodiscovery_http.output
    scrape_interval = "60s"
    honor_labels = true
    bearer_token_file = "/var/run/secrets/kubernetes.io/serviceaccount/token"
    clustering {
      enabled = true
    }

    forward_to = argument.metrics_destinations.value
  }

  prometheus.scrape "annotation_autodiscovery_https" {
    targets = discovery.relabel.annotation_autodiscovery_https.output
    scrape_interval = "60s"
    honor_labels = true
    bearer_token_file = "/var/run/secrets/kubernetes.io/serviceaccount/token"
    tls_config {
      insecure_skip_verify = true
    }
    clustering {
      enabled = true
    }

    forward_to = argument.metrics_destinations.value
  }
}
annotation_autodiscovery "feature" {
  metrics_destinations = [
    otelcol.receiver.prometheus.otlpgateway.receiver,
  ]
}

// Feature: Cluster Metrics
declare "cluster_metrics" {
  argument "metrics_destinations" {
    comment = "Must be a list of metric destinations where collected metrics should be forwarded to"
  }
  import.git "kubernetes" {
    repository = "https://github.com/grafana/alloy-modules.git"
    revision = "main"
    path = "modules/kubernetes/core/metrics.alloy"
    pull_frequency = "15m"
  }  
  
  kubernetes.kubelet "scrape" {
    clustering = true
    keep_metrics = "up|container_cpu_usage_seconds_total|kubelet_certificate_manager_client_expiration_renew_errors|kubelet_certificate_manager_client_ttl_seconds|kubelet_certificate_manager_server_ttl_seconds|kubelet_cgroup_manager_duration_seconds_bucket|kubelet_cgroup_manager_duration_seconds_count|kubelet_node_config_error|kubelet_node_name|kubelet_pleg_relist_duration_seconds_bucket|kubelet_pleg_relist_duration_seconds_count|kubelet_pleg_relist_interval_seconds_bucket|kubelet_pod_start_duration_seconds_bucket|kubelet_pod_start_duration_seconds_count|kubelet_pod_worker_duration_seconds_bucket|kubelet_pod_worker_duration_seconds_count|kubelet_running_container_count|kubelet_running_containers|kubelet_running_pod_count|kubelet_running_pods|kubelet_runtime_operations_errors_total|kubelet_runtime_operations_total|kubelet_server_expiration_renew_errors|kubelet_volume_stats_available_bytes|kubelet_volume_stats_capacity_bytes|kubelet_volume_stats_inodes|kubelet_volume_stats_inodes_used|kubernetes_build_info|namespace_workload_pod|rest_client_requests_total|storage_operation_duration_seconds_count|storage_operation_errors_total|volume_manager_total_volumes"
    scrape_interval = "60s"
    max_cache_size = 100000
    forward_to = argument.metrics_destinations.value
  }  
  
  kubernetes.resources "scrape" {
    clustering = true
    job_label = "integrations/kubernetes/resources"
    keep_metrics = "up|node_cpu_usage_seconds_total|node_memory_working_set_bytes"
    scrape_interval = "60s"
    max_cache_size = 100000
    forward_to = argument.metrics_destinations.value
  }  
  
  kubernetes.cadvisor "scrape" {
    clustering = true
    keep_metrics = "up|container_cpu_cfs_periods_total|container_cpu_cfs_throttled_periods_total|container_cpu_usage_seconds_total|container_fs_reads_bytes_total|container_fs_reads_total|container_fs_writes_bytes_total|container_fs_writes_total|container_memory_cache|container_memory_rss|container_memory_swap|container_memory_working_set_bytes|container_network_receive_bytes_total|container_network_receive_packets_dropped_total|container_network_receive_packets_total|container_network_transmit_bytes_total|container_network_transmit_packets_dropped_total|container_network_transmit_packets_total|machine_memory_bytes"
    scrape_interval = "60s"
    max_cache_size = 100000
    forward_to = [prometheus.relabel.cadvisor.receiver]
  }
  
  prometheus.relabel "cadvisor" {
    max_cache_size = 100000
    // Drop empty container labels, addressing https://github.com/google/cadvisor/issues/2688
    rule {
      source_labels = ["__name__","container"]
      separator = "@"
      regex = "(container_cpu_.*|container_fs_.*|container_memory_.*)@"
      action = "drop"
    }
    // Drop empty image labels, addressing https://github.com/google/cadvisor/issues/2688
    rule {
      source_labels = ["__name__","image"]
      separator = "@"
      regex = "(container_cpu_.*|container_fs_.*|container_memory_.*|container_network_.*)@"
      action = "drop"
    }
    // Normalizing unimportant labels (not deleting to continue satisfying <label>!="" checks)
    rule {
      source_labels = ["__name__", "boot_id"]
      separator = "@"
      regex = "machine_memory_bytes@.*"
      target_label = "boot_id"
      replacement = "NA"
    }
    rule {
      source_labels = ["__name__", "system_uuid"]
      separator = "@"
      regex = "machine_memory_bytes@.*"
      target_label = "system_uuid"
      replacement = "NA"
    }
    // Filter out non-physical devices/interfaces
    rule {
      source_labels = ["__name__", "device"]
      separator = "@"
      regex = "container_fs_.*@(/dev/)?(mmcblk.p.+|nvme.+|rbd.+|sd.+|vd.+|xvd.+|dasd.+)"
      target_label = "__keepme"
      replacement = "1"
    }
    rule {
      source_labels = ["__name__", "__keepme"]
      separator = "@"
      regex = "container_fs_.*@"
      action = "drop"
    }
    rule {
      source_labels = ["__name__"]
      regex = "container_fs_.*"
      target_label = "__keepme"
      replacement = ""
    }
    rule {
      source_labels = ["__name__", "interface"]
      separator = "@"
      regex = "container_network_.*@(en[ospx][0-9].*|wlan[0-9].*|eth[0-9].*)"
      target_label = "__keepme"
      replacement = "1"
    }
    rule {
      source_labels = ["__name__", "__keepme"]
      separator = "@"
      regex = "container_network_.*@"
      action = "drop"
    }
    rule {
      source_labels = ["__name__"]
      regex = "container_network_.*"
      target_label = "__keepme"
      replacement = ""
    }
    forward_to = argument.metrics_destinations.value
  }          
  
  import.git "kube_state_metrics" {
    repository = "https://github.com/grafana/alloy-modules.git"
    revision = "main"
    path = "modules/kubernetes/kube-state-metrics/metrics.alloy"
    pull_frequency = "15m"
  }
  
  kube_state_metrics.kubernetes "targets" {
    label_selectors = [
      "app.kubernetes.io/name=kube-state-metrics",
      "release=ko",
    ]
  }
  
  kube_state_metrics.scrape "metrics" {
    targets = kube_state_metrics.kubernetes.targets.output
    clustering = true
    keep_metrics = "up|kube_daemonset.*|kube_deployment_metadata_generation|kube_deployment_spec_replicas|kube_deployment_status_observed_generation|kube_deployment_status_replicas_available|kube_deployment_status_replicas_updated|kube_horizontalpodautoscaler_spec_max_replicas|kube_horizontalpodautoscaler_spec_min_replicas|kube_horizontalpodautoscaler_status_current_replicas|kube_horizontalpodautoscaler_status_desired_replicas|kube_job.*|kube_namespace_status_phase|kube_node.*|kube_persistentvolumeclaim_resource_requests_storage_bytes|kube_pod_container_info|kube_pod_container_resource_limits|kube_pod_container_resource_requests|kube_pod_container_status_last_terminated_reason|kube_pod_container_status_restarts_total|kube_pod_container_status_waiting_reason|kube_pod_info|kube_pod_owner|kube_pod_start_time|kube_pod_status_phase|kube_pod_status_reason|kube_replicaset.*|kube_resourcequota|kube_statefulset.*"
    scrape_interval = "60s"
    max_cache_size = 100000
    forward_to = argument.metrics_destinations.value
  }  
  
  import.git "node_exporter" {
    repository = "https://github.com/grafana/alloy-modules.git"
    revision = "main"
    path = "modules/system/node-exporter/metrics.alloy"
    pull_frequency = "15m"
  }
  
  node_exporter.kubernetes "targets" {
    label_selectors = [
      "app.kubernetes.io/name=node-exporter",
      "release=ko",
    ]
  }
  
  discovery.relabel "node_exporter" {
    targets = node_exporter.kubernetes.targets.output
    rule {
      source_labels = ["__meta_kubernetes_pod_node_name"]
      action = "replace"
      target_label = "instance"
    }
  }
  
  node_exporter.scrape "metrics" {
    targets = discovery.relabel.node_exporter.output
    job_label = "integrations/node_exporter"
    clustering = true
    keep_metrics = "up|node_cpu.*|node_exporter_build_info|node_filesystem.*|node_memory.*|node_network_receive_bytes_total|node_network_receive_drop_total|node_network_transmit_bytes_total|node_network_transmit_drop_total|process_cpu_seconds_total|process_resident_memory_bytes"
    scrape_interval = "60s"
    max_cache_size = 100000
    forward_to = argument.metrics_destinations.value
  }  
  
  import.git "windows_exporter" {
    repository = "https://github.com/grafana/alloy-modules.git"
    revision = "main"
    path = "modules/system/node-exporter/metrics.alloy"
    pull_frequency = "15m"
  }
  
  windows_exporter.kubernetes "targets" {
    label_selectors = [
      "app.kubernetes.io/name=windows-exporter",
      "release=ko",
    ]
  }
  
  discovery.relabel "windows_exporter" {
    targets = windows_exporter.kubernetes.targets.output
    rule {
      source_labels = ["__meta_kubernetes_pod_node_name"]
      action = "replace"
      target_label = "instance"
    }
  }
  
  windows_exporter.scrape "metrics" {
    targets = discovery.relabel.windows_exporter.output
    clustering = true
    keep_metrics = "up|windows_.*|node_cpu_seconds_total|node_filesystem_size_bytes|node_filesystem_avail_bytes|container_cpu_usage_seconds_total"
    scrape_interval = "60s"
    max_cache_size = 100000
    forward_to = argument.metrics_destinations.value
  }  
  
  discovery.kubernetes "kepler" {
    role = "pod"
    namespaces {
      own_namespace = true
    }
    selectors {
      role = "pod"
      label = "app.kubernetes.io/name=kepler"
    }
  }
  
  discovery.relabel "kepler" {
    targets = discovery.kubernetes.kepler.targets
    rule {
      source_labels = ["__meta_kubernetes_pod_node_name"]
      action = "replace"
      target_label = "instance"
    }
  }
  
  prometheus.scrape "kepler" {
    targets      = discovery.relabel.kepler.output
    job_name     = "integrations/kepler"
    honor_labels = true
    scrape_interval = "60s"
    clustering {
      enabled = true
    }
    forward_to = [prometheus.relabel.kepler.receiver]
  }
  
  prometheus.relabel "kepler" {
    max_cache_size = 100000
    rule {
      source_labels = ["__name__"]
      regex = "up|kepler_.*"
      action = "keep"
    }
    forward_to = argument.metrics_destinations.value
  }
}
cluster_metrics "feature" {
  metrics_destinations = [
    otelcol.receiver.prometheus.otlpgateway.receiver,
  ]
}

// Feature: Prometheus Operator Objects
declare "prometheus_operator_objects" {
  argument "metrics_destinations" {
    comment = "Must be a list of metric destinations where collected metrics should be forwarded to"
  }
  
  // Prometheus Operator podMonitor objects
  prometheus.operator.podmonitors "pod_monitors" {
    clustering {
      enabled = true
    }
    scrape {
      default_scrape_interval = "60s"
    }
    forward_to = argument.metrics_destinations.value
  }
  
  // Prometheus Operator podMonitor objects
  prometheus.operator.probes "pod_monitors" {
    clustering {
      enabled = true
    }
    scrape {
      default_scrape_interval = "60s"
    }
    forward_to = argument.metrics_destinations.value
  }
  
  // Prometheus Operator ServiceMonitor objects
  prometheus.operator.servicemonitors "service_monitors" {
    clustering {
      enabled = true
    }
    scrape {
      default_scrape_interval = "60s"
    }
    forward_to = argument.metrics_destinations.value
  }
}
prometheus_operator_objects "feature" {
  metrics_destinations = [
    otelcol.receiver.prometheus.otlpgateway.receiver,
  ]
}

declare "alloy_integration" {
  argument "metrics_destinations" {
    comment = "Must be a list of metric destinations where collected metrics should be forwarded to"
  }

  declare "alloy_integration_discovery" {
    argument "namespaces" {
      comment = "The namespaces to look for targets in (default: [] is all namespaces)"
      optional = true
    }

    argument "field_selectors" {
      comment = "The field selectors to use to find matching targets (default: [])"
      optional = true
    }

    argument "label_selectors" {
      comment = "The label selectors to use to find matching targets (default: [\"app.kubernetes.io/name=alloy\"])"
      optional = true
    }

    argument "port_name" {
      comment = "The of the port to scrape metrics from (default: http-metrics)"
      optional = true
    }

    // Alloy service discovery for all of the pods
    discovery.kubernetes "alloy_pods" {
      role = "pod"

      selectors {
        role = "pod"
        field = join(coalesce(argument.field_selectors.value, []), ",")
        label = join(coalesce(argument.label_selectors.value, ["app.kubernetes.io/name=alloy"]), ",")
      }

      namespaces {
        names = coalesce(argument.namespaces.value, [])
      }
    }

    // alloy relabelings (pre-scrape)
    discovery.relabel "alloy_pods" {
      targets = discovery.kubernetes.alloy_pods.targets

      // keep only the specified metrics port name, and pods that are Running and ready
      rule {
        source_labels = [
          "__meta_kubernetes_pod_container_port_name",
          "__meta_kubernetes_pod_phase",
          "__meta_kubernetes_pod_ready",
          "__meta_kubernetes_pod_container_init",
        ]
        separator = "@"
        regex = coalesce(argument.port_name.value, "metrics") + "@Running@true@false"
        action = "keep"
      }

      rule {
        source_labels = ["__meta_kubernetes_namespace"]
        target_label  = "namespace"
      }

      rule {
        source_labels = ["__meta_kubernetes_pod_name"]
        target_label  = "pod"
      }

      rule {
        source_labels = ["__meta_kubernetes_pod_container_name"]
        target_label  = "container"
      }

      rule {
        source_labels = [
          "__meta_kubernetes_pod_controller_kind",
          "__meta_kubernetes_pod_controller_name",
        ]
        separator = "/"
        target_label  = "workload"
      }
      // remove the hash from the ReplicaSet
      rule {
        source_labels = ["workload"]
        regex = "(ReplicaSet/.+)-.+"
        target_label  = "workload"
      }

      // set the app name if specified as metadata labels "app:" or "app.kubernetes.io/name:" or "k8s-app:"
      rule {
        action = "replace"
        source_labels = [
          "__meta_kubernetes_pod_label_app_kubernetes_io_name",
          "__meta_kubernetes_pod_label_k8s_app",
          "__meta_kubernetes_pod_label_app",
        ]
        separator = ";"
        regex = "^(?:;*)?([^;]+).*$"
        replacement = "$1"
        target_label = "app"
      }

      // set the component if specified as metadata labels "component:" or "app.kubernetes.io/component:" or "k8s-component:"
      rule {
        action = "replace"
        source_labels = [
          "__meta_kubernetes_pod_label_app_kubernetes_io_component",
          "__meta_kubernetes_pod_label_k8s_component",
          "__meta_kubernetes_pod_label_component",
        ]
        regex = "^(?:;*)?([^;]+).*$"
        replacement = "$1"
        target_label = "component"
      }

      // set a source label
      rule {
        action = "replace"
        replacement = "kubernetes"
        target_label = "source"
      }
    }

    export "output" {
      value = discovery.relabel.alloy_pods.output
    }
  }

  declare "alloy_integration_scrape" {
    argument "targets" {
      comment = "Must be a list() of targets"
    }

    argument "forward_to" {
      comment = "Must be a list(MetricsReceiver) where collected logs should be forwarded to"
    }

    argument "job_label" {
      comment = "The job label to add for all alloy metric (default: integrations/alloy)"
      optional = true
    }

    argument "keep_metrics" {
      comment = "A regular expression of metrics to keep (default: see below)"
      optional = true
    }

    argument "drop_metrics" {
      comment = "A regular expression of metrics to drop (default: see below)"
      optional = true
    }

    argument "scrape_interval" {
      comment = "How often to scrape metrics from the targets (default: 60s)"
      optional = true
    }

    argument "scrape_timeout" {
      comment = "How long before a scrape times out (default: 10s)"
      optional = true
    }

    argument "max_cache_size" {
      comment = "The maximum number of elements to hold in the relabeling cache (default: 100000).  This should be at least 2x-5x your largest scrape target or samples appended rate."
      optional = true
    }

    argument "clustering" {
      comment = "Whether or not clustering should be enabled (default: false)"
      optional = true
    }

    prometheus.scrape "alloy" {
      job_name = coalesce(argument.job_label.value, "integrations/alloy")
      forward_to = [prometheus.relabel.alloy.receiver]
      targets = argument.targets.value
      scrape_interval = coalesce(argument.scrape_interval.value, "60s")
      scrape_timeout = coalesce(argument.scrape_timeout.value, "10s")

      clustering {
        enabled = coalesce(argument.clustering.value, false)
      }
    }

    // alloy metric relabelings (post-scrape)
    prometheus.relabel "alloy" {
      forward_to = argument.forward_to.value
      max_cache_size = coalesce(argument.max_cache_size.value, 100000)

      // drop metrics that match the drop_metrics regex
      rule {
        source_labels = ["__name__"]
        regex = coalesce(argument.drop_metrics.value, "(^(go|process)_.+$)")
        action = "drop"
      }

      // keep only metrics that match the keep_metrics regex
      rule {
        source_labels = ["__name__"]
        regex = coalesce(argument.keep_metrics.value, ".*")
        action = "keep"
      }

      // remove the component_id label from any metric that starts with log_bytes or log_lines, these are custom metrics that are generated
      // as part of the log annotation modules in this repo
      rule {
        action = "replace"
        source_labels = ["__name__"]
        regex = "^log_(bytes|lines).+"
        replacement = ""
        target_label = "component_id"
      }

      // set the namespace label to that of the exported_namespace
      rule {
        action = "replace"
        source_labels = ["__name__", "exported_namespace"]
        separator = "@"
        regex = "^log_(bytes|lines).+@(.+)"
        replacement = "$2"
        target_label = "namespace"
      }

      // set the pod label to that of the exported_pod
      rule {
        action = "replace"
        source_labels = ["__name__", "exported_pod"]
        separator = "@"
        regex = "^log_(bytes|lines).+@(.+)"
        replacement = "$2"
        target_label = "pod"
      }

      // set the container label to that of the exported_container
      rule {
        action = "replace"
        source_labels = ["__name__", "exported_container"]
        separator = "@"
        regex = "^log_(bytes|lines).+@(.+)"
        replacement = "$2"
        target_label = "container"
      }

      // set the job label to that of the exported_job
      rule {
        action = "replace"
        source_labels = ["__name__", "exported_job"]
        separator = "@"
        regex = "^log_(bytes|lines).+@(.+)"
        replacement = "$2"
        target_label = "job"
      }

      // set the instance label to that of the exported_instance
      rule {
        action = "replace"
        source_labels = ["__name__", "exported_instance"]
        separator = "@"
        regex = "^log_(bytes|lines).+@(.+)"
        replacement = "$2"
        target_label = "instance"
      }

      rule {
        action = "labeldrop"
        regex = "exported_(namespace|pod|container|job|instance)"
      }
    }
  }
  
  
  alloy_integration_discovery "alloy" {
    port_name       = "http-metrics"
    label_selectors = ["app.kubernetes.io/name=alloy"]
  }
  
  alloy_integration_scrape  "alloy" {
    targets = alloy_integration_discovery.alloy.output
    clustering = true
    keep_metrics = "up|alloy_build_info"
    scrape_interval = "60s"
    max_cache_size = 100000
    forward_to = argument.metrics_destinations.value
  }
}
alloy_integration "integration" {
  metrics_destinations = [
    otelcol.receiver.prometheus.otlpgateway.receiver,
  ]
}
